{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and evaluation on F3 Netherlands dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seismic interpretation, also referred to as facies classification, is a task of determining types of rock in the earth’s subsurface, given seismic data. Seismic interpretation is used as a standard approach for determining precise locations of oil deposits for drilling, therefore reducing risks and potential losses. In recent years, there has been a great interest in using fully-supervised deep learning models for seismic interpretation. \n",
    "\n",
    "In this notebook, we demonstrate how to train a deep neural network for facies prediction using F3 Netherlands dataset. The F3 block is located in the North Sea off the shores of Netherlands. The dataset contains 6 classes (facies or lithostratigraphic units), all of which are of varying thickness (class imbalance). Processed data is available in numpy format as a `401 x 701 x 255` array. The processed F3 data is made available by [Alaudah et al. 2019](https://github.com/olivesgatech/facies_classification_benchmark). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "To set up the conda environment, please follow the instructions in the top-level [README.md](../../../README.md) file.\n",
    "\n",
    "__Note__: To register the conda environment in Jupyter, run:\n",
    "`python -m ipykernel install --user --name envname`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.config\n",
    "from os import path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "import yacs.config\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from albumentations import Compose, HorizontalFlip, Normalize, PadIfNeeded, Resize\n",
    "from ignite.contrib.handlers import CosineAnnealingScheduler\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.engine import Events\n",
    "from ignite.metrics import Loss\n",
    "from ignite.utils import convert_tensor\n",
    "from toolz import compose\n",
    "from torch.utils import data\n",
    "\n",
    "from cv_lib.utils import load_log_configuration\n",
    "from cv_lib.event_handlers import SnapshotHandler, logging_handlers\n",
    "from cv_lib.segmentation import models, tgs_salt\n",
    "from cv_lib.segmentation.dutchf3.engine import create_supervised_trainer\n",
    "\n",
    "from cv_lib.segmentation.dutchf3.utils import (\n",
    "    current_datetime,\n",
    "    generate_path,\n",
    "    git_branch,\n",
    "    git_hash,\n",
    "    np_to_tb,\n",
    ")\n",
    "\n",
    "from deepseismic_interpretation.dutchf3.data import (\n",
    "    get_patch_loader,\n",
    "    decode_segmap,\n",
    "    get_test_loader,\n",
    ")\n",
    "\n",
    "from utilities import (\n",
    "    plot_aline,\n",
    "    plot_f3block_interactive,\n",
    "    prepare_batch,\n",
    "    _patch_label_2d,\n",
    "    _compose_processing_pipeline,\n",
    "    _output_processing_pipeline,\n",
    "    _write_section_file,\n",
    "    runningScore,\n",
    ")\n",
    "\n",
    "# set device\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "# number of images to score\n",
    "N_EVALUATE = 20\n",
    "# experiment configuration file\n",
    "CONFIG_FILE = \"./configs/patch_deconvnet_skip.yaml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download and preparation\n",
    "\n",
    "To download and prepare the F3 data set, please follow the instructions [here](../../../interpretation/dutchf3_patch/README.md). Once you've downloaded and prepared the data set, you'll find your files in the following directory tree:\n",
    "\n",
    "```\n",
    "data\n",
    "├── splits\n",
    "├── test_once\n",
    "│   ├── test1_labels.npy\n",
    "│   ├── test1_seismic.npy\n",
    "│   ├── test2_labels.npy\n",
    "│   └── test2_seismic.npy\n",
    "└── train\n",
    "    ├── train_labels.npy\n",
    "    └── train_seismic.npy\n",
    "```\n",
    "\n",
    "We recommend saving the data under `/mnt/dutchf3` since this notebook will use that location as the data root. Otherwise, modify the `DATASET.ROOT` field in the configuration file, described next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use configuration files to specify experiment configuration, such as hyperparameters used in training and evaluation, as well as other experiment settings. We provide several configuration files for this notebook, under `./configs`, mainly differing in the DNN architecture used for defining the model.\n",
    "\n",
    "Modify the `CONFIG_FILE` variable above if you would like to run the experiment using a different configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_FILE, \"rt\") as f_read:\n",
    "    config = yacs.config.load_cfg(f_read)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F3 data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a few sections of the F3 data set. The processed F3 data set is stored as a 3D numpy array. Let's view slices of the data along inline and crossline directions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and labels\n",
    "train_seismic = np.load(path.join(config.DATASET.ROOT, \"train/train_seismic.npy\"))\n",
    "train_labels = np.load(path.join(config.DATASET.ROOT, \"train/train_labels.npy\"))\n",
    "\n",
    "print(f\"Number of inline slices: {train_seismic.shape[0]}\")\n",
    "print(f\"Number of crossline slices: {train_seismic.shape[1]}\")\n",
    "print(f\"Depth dimension : {train_seismic.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_f3block_interactive(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot an __inline__ slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 100\n",
    "x_in = train_seismic[idx, :, :].swapaxes(0, 1)\n",
    "x_inl = train_labels[idx, :, :].swapaxes(0, 1)\n",
    "\n",
    "plot_aline(x_in, x_inl, xlabel=\"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a __crossline__ slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cr = train_seismic[:, idx, :].swapaxes(0, 1)\n",
    "x_crl = train_labels[:, idx, :].swapaxes(0, 1)\n",
    "\n",
    "plot_aline(x_cr, x_crl, xlabel=\"crossline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "load_log_configuration(config.LOG_CONFIG)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.debug(config.WORKERS)\n",
    "torch.backends.cudnn.benchmark = config.CUDNN.BENCHMARK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data augmentation\n",
    "\n",
    "Let's define our data augmentation pipeline, which includes basic transformations, such as _data normalization, resizing, and padding_ if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Augmentations\n",
    "basic_aug = Compose(\n",
    "    [\n",
    "        Normalize(\n",
    "            mean=(config.TRAIN.MEAN,), std=(config.TRAIN.STD,), max_pixel_value=1\n",
    "        ),\n",
    "        Resize(\n",
    "            config.TRAIN.AUGMENTATIONS.RESIZE.HEIGHT,\n",
    "            config.TRAIN.AUGMENTATIONS.RESIZE.WIDTH,\n",
    "            always_apply=True,\n",
    "        ),\n",
    "        PadIfNeeded(\n",
    "            min_height=config.TRAIN.AUGMENTATIONS.PAD.HEIGHT,\n",
    "            min_width=config.TRAIN.AUGMENTATIONS.PAD.WIDTH,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            always_apply=True,\n",
    "            mask_value=255,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if config.TRAIN.AUGMENTATION:\n",
    "    train_aug = Compose([basic_aug, HorizontalFlip(p=0.5)])\n",
    "else:\n",
    "    train_aug = basic_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the model, we will use a patch-based approach. Rather than using entire sections (crosslines or inlines) of the data, we extract a large number of small patches from the sections, and use the patches as our data. This allows us to generate larger set of images for training, but is also a more feasible approach for large seismic volumes. \n",
    "\n",
    "We are using a custom patch data loader from our __`deepseismic_interpretation`__ library for generating and loading patches from seismic section data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainPatchLoader = get_patch_loader(config)\n",
    "\n",
    "train_set = TrainPatchLoader(\n",
    "    config.DATASET.ROOT,\n",
    "    split=\"train\",\n",
    "    is_transform=True,\n",
    "    stride=config.TRAIN.STRIDE,\n",
    "    patch_size=config.TRAIN.PATCH_SIZE,\n",
    "    augmentations=train_aug,\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=config.TRAIN.BATCH_SIZE_PER_GPU,\n",
    "    num_workers=config.WORKERS,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a model to train, an optimization algorithm, and a loss function. \n",
    "\n",
    "Note that the model is loaded from our __`cv_lib`__ library, using the name of the model as specified in the configuration file. To load a different model, either change the `MODEL.NAME` field in the configuration file, or create a new one corresponding to the model you wish to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model\n",
    "model = getattr(models, config.MODEL.NAME).get_seg_model(config)\n",
    "\n",
    "# Send to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# SGD optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=config.TRAIN.MAX_LR,\n",
    "    momentum=config.TRAIN.MOMENTUM,\n",
    "    weight_decay=config.TRAIN.WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "# learning rate scheduler\n",
    "scheduler_step = config.TRAIN.END_EPOCH // config.TRAIN.SNAPSHOTS\n",
    "snapshot_duration = scheduler_step * len(train_loader)\n",
    "scheduler = CosineAnnealingScheduler(\n",
    "    optimizer, \"lr\", config.TRAIN.MAX_LR, config.TRAIN.MIN_LR, snapshot_duration\n",
    ")\n",
    "\n",
    "# weights are inversely proportional to the frequency of the classes in the training set\n",
    "class_weights = torch.tensor(\n",
    "    config.DATASET.CLASS_WEIGHTS, device=device, requires_grad=False\n",
    ")\n",
    "\n",
    "# loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(\n",
    "    weight=class_weights, ignore_index=255, reduction=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [ignite](https://pytorch.org/ignite/index.html) framework to create training and validation loops in our codebase. Ignite provides an easy way to create compact training/validation loops without too much boilerplate code.\n",
    "\n",
    "In this notebook, we demonstrate the use of ignite on the training loop only. We create a training engine `trainer` that loops multiple times over the training dataset and updates model parameters. In addition, we add various events to the trainer, using an event system, that allows us to interact with the engine on each step of the run, such as, when the trainer is started/completed, when the epoch is started/completed and so on. \n",
    "\n",
    "In the cell below, we use event handlers to add the following events to the training loop:\n",
    "- log training output\n",
    "- log and schedule learning rate and\n",
    "- periodically save model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training engine\n",
    "trainer = create_supervised_trainer(\n",
    "    model, optimizer, criterion, prepare_batch, device=device\n",
    ")\n",
    "\n",
    "# add learning rate scheduler\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "\n",
    "# add logging of traininig output\n",
    "trainer.add_event_handler(\n",
    "    Events.ITERATION_COMPLETED,\n",
    "    logging_handlers.log_training_output(log_interval=config.PRINT_FREQ),\n",
    ")\n",
    "\n",
    "# add logging of learning rate\n",
    "trainer.add_event_handler(Events.EPOCH_STARTED, logging_handlers.log_lr(optimizer))\n",
    "\n",
    "# add model checkpointing\n",
    "output_dir = path.join(config.OUTPUT_DIR, config.TRAIN.MODEL_DIR)\n",
    "checkpoint_handler = ModelCheckpoint(\n",
    "    output_dir, \"model\", save_interval=2, n_saved=3, create_dir=True, require_empty=False\n",
    ")\n",
    "trainer.add_event_handler(\n",
    "    Events.EPOCH_COMPLETED, checkpoint_handler, {config.MODEL.NAME: model}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training engine run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(train_loader, max_epochs=config.TRAIN.END_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will next evaluate the performance of the model by looking at how well it predicts facies labels on samples from the test set.\n",
    "\n",
    "We will use the following evaluation metrics:\n",
    "\n",
    "- Pixel Accuracy (PA)\n",
    "- Class Accuracy (CA)\n",
    "- Mean Class Accuracy (MCA)\n",
    "- Frequency Weighted intersection-over-union (FW IoU)\n",
    "- Mean IoU (MIoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the model saved previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getattr(models, config.MODEL.NAME).get_seg_model(config)\n",
    "model.load_state_dict(torch.load(config.TEST.MODEL_PATH), strict=False)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the test data and define the augmentations on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation\n",
    "section_aug = Compose(\n",
    "    [Normalize(mean=(config.TRAIN.MEAN,), std=(config.TRAIN.STD,), max_pixel_value=1)]\n",
    ")\n",
    "\n",
    "patch_aug = Compose(\n",
    "    [\n",
    "        Resize(\n",
    "            config.TRAIN.AUGMENTATIONS.RESIZE.HEIGHT,\n",
    "            config.TRAIN.AUGMENTATIONS.RESIZE.WIDTH,\n",
    "            always_apply=True,\n",
    "        ),\n",
    "        PadIfNeeded(\n",
    "            min_height=config.TRAIN.AUGMENTATIONS.PAD.HEIGHT,\n",
    "            min_width=config.TRAIN.AUGMENTATIONS.PAD.WIDTH,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            always_apply=True,\n",
    "            mask_value=255,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Process test data\n",
    "pre_processing = _compose_processing_pipeline(config.TRAIN.DEPTH, aug=patch_aug)\n",
    "output_processing = _output_processing_pipeline(config)\n",
    "\n",
    "# Select the test split\n",
    "split = \"test1\" if \"both\" in config.TEST.SPLIT else config.TEST.SPLIT\n",
    "\n",
    "labels = np.load(path.join(config.DATASET.ROOT, \"test_once\", split + \"_labels.npy\"))\n",
    "section_file = path.join(config.DATASET.ROOT, \"splits\", \"section_\" + split + \".txt\")\n",
    "_write_section_file(labels, section_file, config)\n",
    "\n",
    "# Load test data\n",
    "TestSectionLoader = get_test_loader(config)\n",
    "test_set = TestSectionLoader(\n",
    "    config.DATASET.ROOT, split=split, is_transform=True, augmentations=section_aug\n",
    ")\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "    test_set, batch_size=1, num_workers=config.WORKERS, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict segmentation mask on the test data\n",
    "\n",
    "For demonstration purposes and efficiency, we will only use a subset of the test data to predict segmentation mask on. More precisely, we will score `N_EVALUATE` images. If you would like to evaluate more images, set this variable to the desired number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\n",
    "    \"upper_ns\",\n",
    "    \"middle_ns\",\n",
    "    \"lower_ns\",\n",
    "    \"rijnland_chalk\",\n",
    "    \"scruff\",\n",
    "    \"zechstein\",\n",
    "]\n",
    "\n",
    "n_classes = len(CLASS_NAMES)\n",
    "\n",
    "# keep only N_EVALUATE sections to score\n",
    "test_subset = random.sample(list(test_loader), N_EVALUATE)\n",
    "\n",
    "results = list()\n",
    "running_metrics_split = runningScore(n_classes)\n",
    "\n",
    "# testing mode\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    # loop over testing data\n",
    "    for i, (images, labels) in enumerate(test_subset):\n",
    "        logger.info(f\"split: {split}, section: {i}\")\n",
    "        outputs = _patch_label_2d(\n",
    "            model,\n",
    "            images,\n",
    "            pre_processing,\n",
    "            output_processing,\n",
    "            config.TRAIN.PATCH_SIZE,\n",
    "            config.TEST.TEST_STRIDE,\n",
    "            config.VALIDATION.BATCH_SIZE_PER_GPU,\n",
    "            device,\n",
    "            n_classes,\n",
    "        )\n",
    "\n",
    "        pred = outputs.detach().max(1)[1].numpy()\n",
    "        gt = labels.numpy()\n",
    "        \n",
    "        # update evaluation metrics\n",
    "        running_metrics_split.update(gt, pred)\n",
    "        \n",
    "        # keep ground truth and result for plotting\n",
    "        results.append((np.squeeze(gt), np.squeeze(pred)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the obtained metrics on this subset of test images. Note that we trained our model for for a small number of epochs, for demonstration purposes, so the performance results here are not meant to be representative. \n",
    "\n",
    "The performance exceed the ones shown here when the models are trained properly. For the full report on benchmarking performance results, please refer to the [README.md](../../../README.md) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scores\n",
    "score, _ = running_metrics_split.get_scores()\n",
    "\n",
    "# Log split results\n",
    "print(f'Pixel Acc: {score[\"Pixel Acc: \"]:.3f}')\n",
    "for cdx, class_name in enumerate(CLASS_NAMES):\n",
    "    print(f'  {class_name}_accuracy {score[\"Class Accuracy: \"][cdx]:.3f}')\n",
    "\n",
    "print(f'Mean Class Acc: {score[\"Mean Class Acc: \"]:.3f}')\n",
    "print(f'Freq Weighted IoU: {score[\"Freq Weighted IoU: \"]:.3f}')\n",
    "print(f'Mean IoU: {score[\"Mean IoU: \"]:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the predictions on entire test sections. Note that the crosslines and inlines have different dimensions, however we were able to use them jointly for our network training and evaluation, since we were using smaller patches from the sections, whose size we can control via hyperparameter in the experiment configuration file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,50))\n",
    "\n",
    "nplot = min(N_EVALUATE, 10)\n",
    "for idx in range(nplot):\n",
    "    # plot actual\n",
    "    plt.subplot(nplot, 2, 2*(idx+1)-1)\n",
    "    plt.imshow(results[idx][0])\n",
    "    # plot predicted\n",
    "    plt.subplot(nplot, 2, 2*(idx+1))\n",
    "    plt.imshow(results[idx][1])\n",
    "\n",
    "f_axes = fig.axes\n",
    "_ = f_axes[0].set_title('Actual')\n",
    "_ = f_axes[1].set_title('Predicted')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismic-interpretation",
   "language": "python",
   "name": "seismic-interpretation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
